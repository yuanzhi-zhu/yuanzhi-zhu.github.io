<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>Introduction to Real NVP &mdash; Yuanzhi Zhu</title>
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/vendor/primer-css/css/primer.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/vendor/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/vendor/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/components/collection.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/components/repo-card.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/sections/repo-list.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/sections/mini-repo-list.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/components/boxed-group.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/globals/common.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/vendor/share.js/dist/css/share.min.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/globals/responsive.css">
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/posts/index.css">
    <!-- Latest compiled and minified CSS -->
    

    
    <link rel="canonical" href="https://yuanzhi-zhu.github.io/2022/06/21/Real-NVP-Intro/">
    <link rel="alternate" type="application/atom+xml" title="Yuanzhi Zhu" href="https://yuanzhi-zhu.github.io/feed.xml">
    <!-- <link rel="shortcut icon" href="https://yuanzhi-zhu.github.io/favicon.ico"> -->
    <link rel="shortcut icon" href="https://yuanzhi-zhu.github.io/icon.jpg">
    
    <meta property="og:title" content="Introduction to Real NVP">
      
    <meta name="keywords" content="Computer Vision, Normalizing Flow, Real NVP">
    <meta name="og:keywords" content="Computer Vision, Normalizing Flow, Real NVP">
      
    <meta name="description" content="Prerequisite: Normalizing Flow">
    <meta name="og:description" content="Prerequisite: Normalizing Flow">
      
    
    
        
    
    <meta property="og:url" content="https://yuanzhi-zhu.github.io/2022/06/21/Real-NVP-Intro/">
    <meta property="og:site_name" content="Yuanzhi Zhu">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh_CN" />
    
    <meta property="article:published_time" content="2022-06-21">
    
    <script src="https://yuanzhi-zhu.github.io/assets/vendor/jquery/dist/jquery.min.js"></script>
    <script src="https://yuanzhi-zhu.github.io/assets/js/jquery-ui.js"></script>
    <script src="https://yuanzhi-zhu.github.io/assets/js/main.js"></script>

    
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
          processEnvironments: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          TeX: { equationNumbers: { autoNumber: "AMS" },
               extensions: ["AMSmath.js", "AMSsymbols.js"] }
      },
  "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <!--
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    -->
</head>
<body class="" data-mz="">
    <header class="site-header">
        <div class="container">
            <h1><a href="https://yuanzhi-zhu.github.io/" title="Yuanzhi Zhu"><span class="octicon octicon-mark-github"><font color="black">Yuanzhi Zhu</font></span> </a></h1>
            <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <nav class="site-header-nav" role="navigation">
                
                <a href="https://yuanzhi-zhu.github.io/" class=" site-header-nav-item" target="" title="Home"><font color="black">Home</font></a>
                
                <a href="https://yuanzhi-zhu.github.io/about/" class=" site-header-nav-item" target="" title="About"><font color="black">About</font></a>
                
            </nav>
        </div>
    </header>
    <!-- / header -->

    <!--
<section class="collection-head small geopattern" data-pattern-id="Introduction to">
  <div class="container">
    <div class="columns">
      <div class="column three-fourths">
        <div class="collection-title">
          <h1 class="collection-header">Introduction to Real NVP</h1>
          <div class="collection-info">
            
            <span class="meta-info">
              <span class="octicon octicon-calendar"></span> 2022/06/21
            </span>
             
            <span class="meta-info">
              <span class="octicon octicon-file-directory"></span>
              <a href="https://yuanzhi-zhu.github.io/categories/#Research" title="Research">Research</a>
            </span>
            
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
-->

<!-- / .banner -->
<section class="container content">
<div class="columns">
  <!--
                <div class="column two-thirds">
                    <article class="article-content markdown-body">
                        <p><strong>Prerequisite</strong>: Normalizing Flow</p>

<h1 id="overview">Overview</h1>

<p><strong>Normalizing Flow (NF)</strong> is a kind of generative model just like GANs and VAEs but provides injective mapping between data $X$ and latent variable $Z$ (from a simple distribution) with exact likelihood calculations. Among all the NFs, real NVP is one of the most important, which stands for real-valued non-volume preserving (real NVP) transformation, a set of powerful invertible and learnable transformations.</p>

<p><strong>Target</strong>: The goal here is to build a learnable, reversible transformation between a source domain (distribution) and a latent(simple distribution) domain $Z$ or another target domain. 
Just like most of the other NFs, we can train the real NVP by doing maximum likelihood estimation (MLE).</p>

<p>From now on we just consider the following situation: source domain is images of a style and target domain is Gaussian distribution.</p>

<p>We need to first write the likelihood of source images $p_X(X)$ in a form that is easy to compute.
Given an image $x \in X$, a simple prior probability distribution $p_Z$ on a latent variable $z \in Z$, and a bijection $f : X \rightarrow Z$ (with $g = f^{−1}$ ), the change of variable formula defines a model distribution on $X$ by (as we assume $p_Z$ is Gaussian):</p>

\[p_X(x) = p_Z(f(x))\left| \text{det}\left(\frac{\partial f(x)}{\partial x^T}\right)\right|\]

\[\text{log}(p_X(x)) = \text{log}\left(p_Z(f(x))\right) + \text{log} \left( \left| \text{det}\left(\frac{\partial f(x)}{\partial x^T}\right)\right|\right)\]

<p>where $p_Z(f(x))$ is easy to get given $z=f(x)$, $\mid \text{det}(\frac{\partial f(x)}{\partial x^T})\mid$ is the determinant of Jacobian, which can be construct to be computational efficient, too.</p>

<p>$f$ can be decomposed to many invertible component: $f = f_1 \circ f_2 \circ … \circ f_n$ just like deep neural network.</p>

<p>We describe here a batch of images as tensor of size $\text{batch}.\text{size}() = (B,C,H,W)$.</p>

<p>For simplicity, we set batch_size $B=1$. 
For any input image, we expect to get a latent variable $z\sim N(0,1)$. In order to calculate $\text{log} p_X(x)$, the model is also required to keep track the sum of log determinant of Jacobian (sldj) $J = \log \det \frac{\partial f(x)}{\partial x^T}$.</p>

<h1 id="pre-process">Pre-process</h1>

<p>We further assume the value of each element in this tensor $\in [0,1]$.</p>

<p>The next step is to de-quantize these images, to make the values a continues distribution. (Since the discrete data distribution has differential entropy of negative infinity, this can lead to arbitrary high likelihoods even on test data. To avoid this case, it is becoming best practice to add real-valued noise to the integer pixel values to de-quantize the data):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mf">255.</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="mf">256.</span>      <span class="c1"># [0,1] with noise
</span></code></pre></div></div>
<p>Notice: now for the same input images, the model will give each time slightly different $z\in Z$.</p>

<p>According to https://arxiv.org/abs/1605.08803, Section 4.1, we will model logits:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">log</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">x</span><span class="p">).</span><span class="n">log</span><span class="p">()</span>            
</code></pre></div></div>
<p>Since $x\in [0,1]$, it’s dangerous to apply directly log to them, so we apply a data_constraint $\beta=0.9$ to $x$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert to logits
</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_constraint</span>  <span class="c1"># [-0.9, 0.9]
</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>                         <span class="c1"># [0.05, 0.95]
</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">log</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">y</span><span class="p">).</span><span class="n">log</span><span class="p">()</span>            <span class="c1"># logit: y=log((((2x-1)beta+1)/2)/(1-((2x-1)beta+1)/2))          
</span></code></pre></div></div>

<p>Now let compute the sldj of pre-process step.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Save log-determinant of Jacobian of initial transform
# Softplus(x) = 1/beta*log(1+exp(beta*x)) (default beta=1) element-wise: activation function
</span><span class="n">ldj</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> \
    <span class="o">-</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">((</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_constraint</span><span class="p">).</span><span class="n">log</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_constraint</span><span class="p">.</span><span class="n">log</span><span class="p">())</span>
<span class="c1"># sum up pixels and channels of every in each batch
</span><span class="n">ldj</span> <span class="o">=</span> <span class="n">ldj</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">ldj</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>As commented above, $\text{F.softplus}(x) = \text{log}(1+e^{x})$, and it’s obvious that $\text{F.softplus}(\text{log}(1-\beta)-\text{log}(\beta)) = \text{log}(1/\beta)$.</p>

<p>For $\text{F.softplus}(y) + \text{F.softplus}(-y)$, the reader can verify it is correct themselves or go through the next few lines.</p>

\[\text{F.softplus}(y) + \text{F.softplus}(-y)\\
=\text{F.softplus}\left( \text{log}\left( \frac{1+(2x-1)\beta}{1-(2x-1)\beta} \right) \right) + \text{F.softplus}\left(- \text{log}\left(\frac{1+(2x-1)\beta}{1-(2x-1)\beta} \right) \right)\\
=\text{log}\left( 1+\frac{1+(2x-1)\beta}{1-(2x-1)\beta} \right) + \text{log}\left( 1+\frac{1-(2x-1)\beta}{1+(2x-1)\beta} \right) \\
= \text{log}\left( \frac{2}{1-(2x-1)\beta} \right) + \text{log}\left( \frac{2}{1+(2x-1)\beta} \right) = \frac{\partial y}{\partial x} * \frac{1}{\beta}\]

<h1 id="real-nvp-model">Real NVP model</h1>

<h2 id="coupling-layer">Coupling Layer</h2>
<p>The very key component of Real NVP is the coupling layer, which is used to passing information while keep the sldj easy to compute. The original figure of coupling from the paper illustrates the forward and inverse operation of this coupling layer. In this coupling layer, the input $x$ is split into two $x_1$ and $x_2$ by channel, and then propagates according to this figure.
<img src="/images/blog/real_NVP/coupling-layer.png" alt="" /></p>

<p>Indeed, this figure is about the so called affine coupling layer (with scale and bias terms), where the functions $s$ and $t$ can be arbitrary complex neural networks (even attention module or with additional/conditional information injected).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># split x
</span><span class="n">x_change</span><span class="p">,</span> <span class="n">x_id</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">st</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">nn</span><span class="p">(</span><span class="n">x_id</span><span class="p">)</span>
<span class="c1"># apply the nn in a checkerboard manner
</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">st</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">...],</span> <span class="n">st</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">...]</span>
<span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="c1"># Scale and translate
</span><span class="k">if</span> <span class="n">reverse</span><span class="p">:</span>
    <span class="n">x_change</span> <span class="o">=</span> <span class="n">x_change</span> <span class="o">*</span> <span class="n">s</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">exp</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span>
    <span class="c1"># track ldj
</span>    <span class="n">ldj</span> <span class="o">=</span> <span class="n">ldj</span> <span class="o">-</span> <span class="n">s</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">x_change</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_change</span> <span class="o">+</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">s</span><span class="p">.</span><span class="n">exp</span><span class="p">()</span>
    <span class="c1"># track ldj
</span>    <span class="n">ldj</span> <span class="o">=</span> <span class="n">ldj</span> <span class="o">+</span> <span class="n">s</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># concatenate recover x
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x_change</span><span class="p">,</span> <span class="n">x_id</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>This structure is invertible because $y_1=x_1$ is guaranteed for both forward and inverse propagation. On the other hand, this also means that the we can’t keep those channels unchanged all the time. One way to do so is to swap the position of $x_1$ and $x_2$ recursively like in the figure below. (in Glow model, the $1\times 1$ invertible convolution is introduced to better mitigate this issue.)</p>

<p><img src="/images/blog/real_NVP/alternating-pattern.png" alt="" /></p>

<h2 id="squeezing-and-masking">Squeezing and Masking</h2>
<p>Before the coupling layer, the checkerboard masking and squeezing is introduced, as shown in the following figure:
<img src="/images/blog/real_NVP/squeeze-mask.png" alt="" /></p>

<p>As you may get from this figure, each sub-channel after squeezing is just a scaled smaller image. This makes suer that $x_1$ and $x_2$ contain the information from the original image evenly.</p>

<p>According to the original paper: The squeezing operation reduces the $4 × 4 × 1$ tensor (on the left) into a $2 × 2 × 4$ tensor (on the right). Before the squeezing operation, a checkerboard pattern is used for coupling layers while a channel-wise masking pattern is used afterward.</p>

<p>These operations are both invertible because they are both linear operations.</p>

<h2 id="multi-scale-architecture">Multi-scale Architecture</h2>
<p>The squeeze operation mentioned above enable a multi-scale architecture used in the Real NVP, as shown in this picture. The main purpose of this architecture is to save computational resources (GPU memory etc.). When initializing the model, each scale is initialized recursively.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># recursively build real NVP scales 
</span><span class="k">if</span> <span class="n">num_scales</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">next</span> <span class="o">=</span> <span class="n">_RealNVP</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">num_scales</span><span class="o">=</span><span class="n">num_scales</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/images/blog/real_NVP/multi-scale.png" alt="" /></p>

<p>As you can see in the figure, in each scale the size of the image is halved but with full connection to the next scale(information passing).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># split
</span><span class="n">x</span><span class="p">,</span> <span class="n">x_split</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">sldj</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sldj</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>
<span class="c1"># cat
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">x_split</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p>Every time after the squeeze, $x$ is split into $x$ and $x_{split}$, where $x$ will be processed in the following scale blocks and $x_{split}$ will remain unchanged. After $x$ is processed, it’s concatenated with $x_{split}$ to form the output with same size.</p>

                    </article>
                    <div class="share">
                        <div class="share-component"></div>
                    </div>
                    <div class="comment">
                        

  

  
        <div id="gitalk-container"></div>
        <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
        <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
        <script>
        var gitalk = new Gitalk({
            id: '/2022/06/21/Real-NVP-Intro/',
            clientID: '7fdcca9624f082f2fd1e',
            clientSecret: '7408a6bc70b766c85f3eaff22433262e00c848e2',
            repo: 'yuanzhi-zhu',
            owner: 'yuanzhi-zhu',
            admin: ['yuanzhi-zhu'],
            labels: ['gitment'],
            perPage: 50,
        })
        gitalk.render('gitalk-container')
        </script>
  


                    </div>
                </div>
                <div class="column one-third">
                    
<h3>Search</h3>
<div id="site_search">
    <input type="text" id="search_box" placeholder="Search">
</div>

<ul id="search_results"></ul>

<link rel="stylesheet" type="text/css" href="https://yuanzhi-zhu.github.io/assets/css/modules/sidebar-search.css">
<script src="https://yuanzhi-zhu.github.io/assets/js/simple-jekyll-search.min.js"></script>
<script src="https://yuanzhi-zhu.github.io/assets/js/search.js"></script>

<script type="text/javascript">
SimpleJekyllSearch({
    searchInput: document.getElementById('search_box'),
    resultsContainer: document.getElementById('search_results'),
    json: 'https://yuanzhi-zhu.github.io/assets/search_data.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a></li>',
    noResultsText: 'No results found',
    limit: 10,
    fuzzy: false,
    exclude: ['Welcome']
})
</script>
 
 <h3>My Popular Repositories</h3>



<a href="https://github.com/yuanzhi-zhu/ETH_PAI_MindMap" target="_blank" class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="ETH_PAI_MindMap">
            <div class="card-image-cell">
                <h3 class="card-title">
                    ETH_PAI_MindMap
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text">Mind map for ETH course Probabilistic Artificial Intelligence(PAI) </p>
            </div>
            <div class="card-text">
                <span class="meta-info" title="4 stars">
                    <span class="octicon octicon-star"></span> 4
                </span>
                <span class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span class="meta-info" title="Last updated: 2022-05-13 15:58:43 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2022-05-13 15:58:43 UTC">2022-05-13</time>
                </span>
            </div>
        </div>
    </div>
</a>

<a href="https://github.com/yuanzhi-zhu/yuanzhi-zhu.github.io" target="_blank" class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="yuanzhi-zhu.github.io">
            <div class="card-image-cell">
                <h3 class="card-title">
                    yuanzhi-zhu.github.io
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text">Personal blog</p>
            </div>
            <div class="card-text">
                <span class="meta-info" title="2 stars">
                    <span class="octicon octicon-star"></span> 2
                </span>
                <span class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span class="meta-info" title="Last updated: 2022-07-14 20:50:31 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2022-07-14 20:50:31 UTC">2022-07-14</time>
                </span>
            </div>
        </div>
    </div>
</a>

<a href="https://github.com/yuanzhi-zhu/CSNLP-Project-ETH" target="_blank" class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="CSNLP-Project-ETH">
            <div class="card-image-cell">
                <h3 class="card-title">
                    CSNLP-Project-ETH
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text">This is the repo for our computational semantics natural language processing course project</p>
            </div>
            <div class="card-text">
                <span class="meta-info" title="1 stars">
                    <span class="octicon octicon-star"></span> 1
                </span>
                <span class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span class="meta-info" title="Last updated: 2022-07-15 19:11:18 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2022-07-15 19:11:18 UTC">2022-07-15</time>
                </span>
            </div>
        </div>
    </div>
</a>



                </div>
            -->
  <article class="article-content markdown-body">
    <p><strong>Prerequisite</strong>: Normalizing Flow</p>

<h1 id="overview">Overview</h1>

<p><strong>Normalizing Flow (NF)</strong> is a kind of generative model just like GANs and VAEs but provides injective mapping between data $X$ and latent variable $Z$ (from a simple distribution) with exact likelihood calculations. Among all the NFs, real NVP is one of the most important, which stands for real-valued non-volume preserving (real NVP) transformation, a set of powerful invertible and learnable transformations.</p>

<p><strong>Target</strong>: The goal here is to build a learnable, reversible transformation between a source domain (distribution) and a latent(simple distribution) domain $Z$ or another target domain. 
Just like most of the other NFs, we can train the real NVP by doing maximum likelihood estimation (MLE).</p>

<p>From now on we just consider the following situation: source domain is images of a style and target domain is Gaussian distribution.</p>

<p>We need to first write the likelihood of source images $p_X(X)$ in a form that is easy to compute.
Given an image $x \in X$, a simple prior probability distribution $p_Z$ on a latent variable $z \in Z$, and a bijection $f : X \rightarrow Z$ (with $g = f^{−1}$ ), the change of variable formula defines a model distribution on $X$ by (as we assume $p_Z$ is Gaussian):</p>

\[p_X(x) = p_Z(f(x))\left| \text{det}\left(\frac{\partial f(x)}{\partial x^T}\right)\right|\]

\[\text{log}(p_X(x)) = \text{log}\left(p_Z(f(x))\right) + \text{log} \left( \left| \text{det}\left(\frac{\partial f(x)}{\partial x^T}\right)\right|\right)\]

<p>where $p_Z(f(x))$ is easy to get given $z=f(x)$, $\mid \text{det}(\frac{\partial f(x)}{\partial x^T})\mid$ is the determinant of Jacobian, which can be construct to be computational efficient, too.</p>

<p>$f$ can be decomposed to many invertible component: $f = f_1 \circ f_2 \circ … \circ f_n$ just like deep neural network.</p>

<p>We describe here a batch of images as tensor of size $\text{batch}.\text{size}() = (B,C,H,W)$.</p>

<p>For simplicity, we set batch_size $B=1$. 
For any input image, we expect to get a latent variable $z\sim N(0,1)$. In order to calculate $\text{log} p_X(x)$, the model is also required to keep track the sum of log determinant of Jacobian (sldj) $J = \log \det \frac{\partial f(x)}{\partial x^T}$.</p>

<h1 id="pre-process">Pre-process</h1>

<p>We further assume the value of each element in this tensor $\in [0,1]$.</p>

<p>The next step is to de-quantize these images, to make the values a continues distribution. (Since the discrete data distribution has differential entropy of negative infinity, this can lead to arbitrary high likelihoods even on test data. To avoid this case, it is becoming best practice to add real-valued noise to the integer pixel values to de-quantize the data):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mf">255.</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="mf">256.</span>      <span class="c1"># [0,1] with noise
</span></code></pre></div></div>
<p>Notice: now for the same input images, the model will give each time slightly different $z\in Z$.</p>

<p>According to https://arxiv.org/abs/1605.08803, Section 4.1, we will model logits:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">log</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">x</span><span class="p">).</span><span class="n">log</span><span class="p">()</span>            
</code></pre></div></div>
<p>Since $x\in [0,1]$, it’s dangerous to apply directly log to them, so we apply a data_constraint $\beta=0.9$ to $x$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert to logits
</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_constraint</span>  <span class="c1"># [-0.9, 0.9]
</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>                         <span class="c1"># [0.05, 0.95]
</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">log</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">y</span><span class="p">).</span><span class="n">log</span><span class="p">()</span>            <span class="c1"># logit: y=log((((2x-1)beta+1)/2)/(1-((2x-1)beta+1)/2))          
</span></code></pre></div></div>

<p>Now let compute the sldj of pre-process step.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Save log-determinant of Jacobian of initial transform
# Softplus(x) = 1/beta*log(1+exp(beta*x)) (default beta=1) element-wise: activation function
</span><span class="n">ldj</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> \
    <span class="o">-</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">((</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_constraint</span><span class="p">).</span><span class="n">log</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_constraint</span><span class="p">.</span><span class="n">log</span><span class="p">())</span>
<span class="c1"># sum up pixels and channels of every in each batch
</span><span class="n">ldj</span> <span class="o">=</span> <span class="n">ldj</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">ldj</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>As commented above, $\text{F.softplus}(x) = \text{log}(1+e^{x})$, and it’s obvious that $\text{F.softplus}(\text{log}(1-\beta)-\text{log}(\beta)) = \text{log}(1/\beta)$.</p>

<p>For $\text{F.softplus}(y) + \text{F.softplus}(-y)$, the reader can verify it is correct themselves or go through the next few lines.</p>

\[\text{F.softplus}(y) + \text{F.softplus}(-y)\\
=\text{F.softplus}\left( \text{log}\left( \frac{1+(2x-1)\beta}{1-(2x-1)\beta} \right) \right) + \text{F.softplus}\left(- \text{log}\left(\frac{1+(2x-1)\beta}{1-(2x-1)\beta} \right) \right)\\
=\text{log}\left( 1+\frac{1+(2x-1)\beta}{1-(2x-1)\beta} \right) + \text{log}\left( 1+\frac{1-(2x-1)\beta}{1+(2x-1)\beta} \right) \\
= \text{log}\left( \frac{2}{1-(2x-1)\beta} \right) + \text{log}\left( \frac{2}{1+(2x-1)\beta} \right) = \frac{\partial y}{\partial x} * \frac{1}{\beta}\]

<h1 id="real-nvp-model">Real NVP model</h1>

<h2 id="coupling-layer">Coupling Layer</h2>
<p>The very key component of Real NVP is the coupling layer, which is used to passing information while keep the sldj easy to compute. The original figure of coupling from the paper illustrates the forward and inverse operation of this coupling layer. In this coupling layer, the input $x$ is split into two $x_1$ and $x_2$ by channel, and then propagates according to this figure.
<img src="/images/blog/real_NVP/coupling-layer.png" alt="" /></p>

<p>Indeed, this figure is about the so called affine coupling layer (with scale and bias terms), where the functions $s$ and $t$ can be arbitrary complex neural networks (even attention module or with additional/conditional information injected).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># split x
</span><span class="n">x_change</span><span class="p">,</span> <span class="n">x_id</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">st</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">nn</span><span class="p">(</span><span class="n">x_id</span><span class="p">)</span>
<span class="c1"># apply the nn in a checkerboard manner
</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">st</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">...],</span> <span class="n">st</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">...]</span>
<span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="c1"># Scale and translate
</span><span class="k">if</span> <span class="n">reverse</span><span class="p">:</span>
    <span class="n">x_change</span> <span class="o">=</span> <span class="n">x_change</span> <span class="o">*</span> <span class="n">s</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">exp</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span>
    <span class="c1"># track ldj
</span>    <span class="n">ldj</span> <span class="o">=</span> <span class="n">ldj</span> <span class="o">-</span> <span class="n">s</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">x_change</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_change</span> <span class="o">+</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">s</span><span class="p">.</span><span class="n">exp</span><span class="p">()</span>
    <span class="c1"># track ldj
</span>    <span class="n">ldj</span> <span class="o">=</span> <span class="n">ldj</span> <span class="o">+</span> <span class="n">s</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># concatenate recover x
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x_change</span><span class="p">,</span> <span class="n">x_id</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>This structure is invertible because $y_1=x_1$ is guaranteed for both forward and inverse propagation. On the other hand, this also means that the we can’t keep those channels unchanged all the time. One way to do so is to swap the position of $x_1$ and $x_2$ recursively like in the figure below. (in Glow model, the $1\times 1$ invertible convolution is introduced to better mitigate this issue.)</p>

<p><img src="/images/blog/real_NVP/alternating-pattern.png" alt="" /></p>

<h2 id="squeezing-and-masking">Squeezing and Masking</h2>
<p>Before the coupling layer, the checkerboard masking and squeezing is introduced, as shown in the following figure:
<img src="/images/blog/real_NVP/squeeze-mask.png" alt="" /></p>

<p>As you may get from this figure, each sub-channel after squeezing is just a scaled smaller image. This makes suer that $x_1$ and $x_2$ contain the information from the original image evenly.</p>

<p>According to the original paper: The squeezing operation reduces the $4 × 4 × 1$ tensor (on the left) into a $2 × 2 × 4$ tensor (on the right). Before the squeezing operation, a checkerboard pattern is used for coupling layers while a channel-wise masking pattern is used afterward.</p>

<p>These operations are both invertible because they are both linear operations.</p>

<h2 id="multi-scale-architecture">Multi-scale Architecture</h2>
<p>The squeeze operation mentioned above enable a multi-scale architecture used in the Real NVP, as shown in this picture. The main purpose of this architecture is to save computational resources (GPU memory etc.). When initializing the model, each scale is initialized recursively.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># recursively build real NVP scales 
</span><span class="k">if</span> <span class="n">num_scales</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">next</span> <span class="o">=</span> <span class="n">_RealNVP</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">num_scales</span><span class="o">=</span><span class="n">num_scales</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/images/blog/real_NVP/multi-scale.png" alt="" /></p>

<p>As you can see in the figure, in each scale the size of the image is halved but with full connection to the next scale(information passing).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># split
</span><span class="n">x</span><span class="p">,</span> <span class="n">x_split</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">sldj</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sldj</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>
<span class="c1"># cat
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">x_split</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p>Every time after the squeeze, $x$ is split into $x$ and $x_{split}$, where $x$ will be processed in the following scale blocks and $x_{split}$ will remain unchanged. After $x$ is processed, it’s concatenated with $x_{split}$ to form the output with same size.</p>

  </article>
  <div class="comment">
    

  

  
        <div id="gitalk-container"></div>
        <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
        <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
        <script>
        var gitalk = new Gitalk({
            id: '/2022/06/21/Real-NVP-Intro/',
            clientID: '7fdcca9624f082f2fd1e',
            clientSecret: '7408a6bc70b766c85f3eaff22433262e00c848e2',
            repo: 'yuanzhi-zhu',
            owner: 'yuanzhi-zhu',
            admin: ['yuanzhi-zhu'],
            labels: ['gitment'],
            perPage: 50,
        })
        gitalk.render('gitalk-container')
        </script>
  


  </div>
</div>
</section>
<!-- /section.content -->

    <footer class="container">
        <div class="site-footer" role="contentinfo">
            <div class="copyright left mobile-block">
                    © 2019 Oct.
                    <span title="Yuanzhi Zhu">Yuanzhi Zhu</span>
                    <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
            </div>

            <ul class="site-footer-links right mobile-hidden">
                <li>
                    <a href="javascript:window.scrollTo(0,0)" >TOP</a>
                </li>
            </ul>
            <a href="https://github.com/yuanzhi-zhu/yuanzhi-zhu.github.io" target="_blank" aria-label="view source code">
                <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
            </a>
            <ul class="site-footer-links mobile-hidden">
                
                <li>
                    <a href="https://yuanzhi-zhu.github.io/" title="Home" target="">Home</a>
                </li>
                
                <li>
                    <a href="https://yuanzhi-zhu.github.io/about/" title="About" target="">About</a>
                </li>
                
                <li><a href="https://yuanzhi-zhu.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li>
            </ul>

        </div>
    </footer>
    <div class="tools-wrapper">
      <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a>
    </div>
    <!-- / footer -->
    <script src="https://yuanzhi-zhu.github.io/assets/vendor/share.js/dist/js/share.min.js"></script>
    <script src="https://yuanzhi-zhu.github.io/assets/js/geopattern.js"></script>
    <script src="https://yuanzhi-zhu.github.io/assets/js/prism.js"></script>
    <link rel="stylesheet" href="https://yuanzhi-zhu.github.io/assets/css/globals/prism.css">
    <script>
      jQuery(document).ready(function($) {
        // geopattern
        $('.geopattern').each(function(){
          $(this).geopattern($(this).data('pattern-id'));
        });
       // hljs.initHighlightingOnLoad();
      });
    </script>

    

    

    

    <!--
    
    <script type="text/javascript" src="https://basis-learning.github.io/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    -->
    
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
          processEnvironments: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          TeX: { equationNumbers: { autoNumber: "AMS" },
               extensions: ["AMSmath.js", "AMSsymbols.js"] }
      },
  "HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

    
</body>
</html>
